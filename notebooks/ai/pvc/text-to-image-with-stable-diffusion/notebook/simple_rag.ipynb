{
 "cells": [
  {
   "cell_type": "raw",
   "id": "670a2599-f513-4b5c-bc67-6ed621e7acd6",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0\n",
    "Copyright (c) 2023, Eduardo Alvarez <eduardo.a.alvarez@intel.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0950f7-2a5a-4c44-9aa0-9938214ede24",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG) with LangChain on the Intel Developer Cloud\n",
    "\n",
    "### 1. Retrieval Augmented Generation (RAG):\n",
    "Retrieval Augmented Generation (RAG) is a novel approach that combines the strengths of large-scale retrieval systems with the generative capabilities of transformers like GPT. In a RAG-based system, when a question is posed, relevant documents or passages are retrieved from a corpus, and then fed alongside the query to a generative model. This two-step process enables the model to leverage both external knowledge from the corpus and its internal knowledge to produce more informed and contextually accurate responses.\n",
    "\n",
    "### 2. In-context Learning:\n",
    "Traditional machine learning models learn from extensive labeled datasets. In contrast, in-context learning pertains to models, especially language models, leveraging a few examples or context provided at inference time to tailor their outputs. This technique allows LLMs to be dynamically adapted to new tasks without undergoing explicit fine-tuning.\n",
    "\n",
    "### 3. LLM Chains/Pipelines:\n",
    "LLM chains or pipelines involve stringing together multiple stages or components of a system to achieve a complex task. For instance, in a RAG system, a retrieval component fetches relevant data from a database, followed by a generation component that constructs the final answer. By chaining different LLM modules, developers can build systems that harness diverse capabilities and can be modularly updated or optimized.\n",
    "\n",
    "### 4. RAG for On-Premise LLM Applications:\n",
    "With the growing need for data privacy and proprietary data handling, many enterprises seek solutions to harness the power of LLMs in-house. RAG provides a unique opportunity for such use-cases. By integrating RAG with on-premise data repositories, enterprises can build powerful LLM applications tailored to their specific needs, while ensuring data confidentiality.\n",
    "\n",
    "### 5. RAG vs Fine-Tuning:\n",
    "While RAG is a powerful approach on its own, it can also be combined with fine-tuning to enhance LLM capabilities further. Fine-tuning allows models to be explicitly trained on specific datasets, honing them for certain tasks. When coupled with RAG, fine-tuned models can make informed decisions using retrieved external knowledge, yielding even more precise and task-specific results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f409d0b8-ca60-45fd-b183-105217d3a539",
   "metadata": {},
   "source": [
    "#### Install dependencies. Only run the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91658eca-f4c4-4a66-aae6-6dda4e55273c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "#!{sys.executable} -m pip install torch==2.2.0 --no-warn-script-location > /dev/null\n",
    "#!{sys.executable} -m pip install langchain==0.0.335 --no-warn-script-location > /dev/null\n",
    "#!{sys.executable} -m pip install pygpt4all==1.1.0 --no-warn-script-location > /dev/null\n",
    "#!{sys.executable} -m pip install gpt4all==1.0.12 --no-warn-script-location > /dev/null\n",
    "#!{sys.executable} -m pip install transformers==4.35.1 --no-warn-script-location > /dev/null\n",
    "#!{sys.executable} -m pip install datasets==2.14.6 --no-warn-script-location > /dev/null\n",
    "#!{sys.executable} -m pip install tiktoken==0.4.0 --no-warn-script-location > /dev/null\n",
    "#!{sys.executable} -m pip install chromadb==0.4.15 --no-warn-script-location > /dev/null\n",
    "#!{sys.executable} -m pip install sentence_transformers==2.2.2 --no-warn-script-location > /dev/null\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d18bcb-2fa0-4a85-ab6d-8bdfc9e2c7db",
   "metadata": {},
   "source": [
    "The code below builds RAGBot. RAGBot class is a streamlined implementation of a Retrieval-Augmented Generation (RAG) chatbot, designed to integrate large language models for generating contextually relevant responses. At its core, it manages the downloading of specific language models, such as the \"Falcon\" model, with support for handling large model files. It also automates the process of fetching and structuring dialogue datasets from predefined sources. The chatbot utilizes the GPT4All model, which is adjustable for various operational parameters like the number of threads and maximum tokens, to enhance response generation efficiency. A key feature of RAGBot is its ability to build a vector database for text retrieval, significantly bolstering its ability to pull relevant document snippets based on user queries. This functionality, combined with a retrieval mechanism and an inference method, makes RAGBot a simple tool for developers aiming to learn about RAG and leverage this implementation as a basis for their implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "887675dc-9560-412d-98c3-93ab9b502a16",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import contextlib\n",
    "import pandas as pd\n",
    "import time\n",
    "import io\n",
    "\n",
    "from tqdm import tqdm\n",
    "from langchain.llms import GPT4All\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "class RAGBot:\n",
    "    \"\"\"\n",
    "    A class to handle model downloading, dataset management, model loading, vector database\n",
    "    creation, retrieval mechanisms, and inference for a response generation bot.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    model_path : str\n",
    "        The file path where the model is stored.\n",
    "    data_path : str\n",
    "        The file path where the dataset is stored.\n",
    "    user_input : str\n",
    "        The input provided by the user for generating a response.\n",
    "    model : str\n",
    "        The name of the model being used.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the RAGBot with default values for model path, data path,\n",
    "        user input, and model.\n",
    "        \"\"\"\n",
    "        self.model_path = \"\"\n",
    "        self.data_path = \"\"\n",
    "        self.user_input = \"\"\n",
    "        self.model = \"\"\n",
    "\n",
    "    def get_model(self, model, chunk_size: int = 10000):\n",
    "        \"\"\"\n",
    "        Downloads the specified model to the model path. Supports downloading of large\n",
    "        models in chunks.\n",
    "\n",
    "        Additional download tooling is reserved for users to add their own models. Currently hardcoded to load Falcon from \n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : str\n",
    "            The name of the model to be downloaded.\n",
    "        chunk_size : int, optional\n",
    "            The size of each chunk of data to download at a time, by default 10000.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "        if self.model == \"Falcon\":\n",
    "            self.model_path = \"/home/common/data/Big_Data/GenAI/llm_models/nomic-ai--gpt4all-falcon-ggml/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "        elif model == \"More Models Coming Soon!\":\n",
    "            print(\"More models coming soon, defaulting to Falcon for now!\")\n",
    "            self.model_path = \"/home/common/data/Big_Data/GenAI/llm_models/nomic-ai--gpt4all-falcon-ggml/ggml-model-gpt4all-falcon-q4_0.bin\"\n",
    "\n",
    "        if not os.path.isfile(self.model_path):\n",
    "            # send a GET request to the URL to download the file. Stream since it's large\n",
    "            response = requests.get(url, stream=True)\n",
    "            # open the file in binary mode and write the contents of the response to it in chunks\n",
    "            # This is a large file, so be prepared to wait.\n",
    "            with open(self.model_path, 'wb') as f:\n",
    "                for chunk in tqdm(response.iter_content(chunk_size=10000)):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        else:\n",
    "            print('model already exists in path.')\n",
    "\n",
    "    def download_dataset(self, dataset):\n",
    "        \"\"\"\n",
    "        Downloads the specified dataset and saves it to the data path.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : str\n",
    "            The name of the dataset to be downloaded.\n",
    "        \"\"\"\n",
    "        self.data_path = dataset + '_dialogues.txt'\n",
    "\n",
    "        if not os.path.isfile(self.data_path):\n",
    "\n",
    "            datasets = {\"robot maintenance\": \"FunDialogues/customer-service-robot-support\", \n",
    "                        \"basketball coach\": \"FunDialogues/sports-basketball-coach\", \n",
    "                        \"physics professor\": \"FunDialogues/academia-physics-office-hours\",\n",
    "                        \"grocery cashier\" : \"FunDialogues/customer-service-grocery-cashier\"}\n",
    "            \n",
    "            # Download the dialogue from hugging face\n",
    "            dataset = load_dataset(f\"{datasets[dataset]}\")\n",
    "            # Convert the dataset to a pandas dataframe\n",
    "            dialogues = dataset['train']\n",
    "            df = pd.DataFrame(dialogues, columns=['id', 'description', 'dialogue'])\n",
    "            # Print the first 5 rows of the dataframe\n",
    "            df.head()\n",
    "            # only keep the dialogue column\n",
    "            dialog_df = df['dialogue']\n",
    "            \n",
    "            # save the data to txt file\n",
    "            dialog_df.to_csv(self.data_path, sep=' ', index=False)\n",
    "        else:\n",
    "            print('data already exists in path.')        \n",
    "\n",
    "    def load_model(self, n_threads, max_tokens, repeat_penalty, n_batch, top_k, temp):\n",
    "        \"\"\"\n",
    "        Loads the model with specified parameters for parallel processing.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_threads : int\n",
    "            The number of threads for parallel processing.\n",
    "        max_tokens : int\n",
    "            The maximum number of tokens for model prediction.\n",
    "        repeat_penalty : float\n",
    "            The penalty for repeated tokens in generation.\n",
    "        n_batch : int\n",
    "            The number of batches for processing.\n",
    "        top_k : int\n",
    "            The number of top k tokens to be considered in sampling.\n",
    "        \"\"\"\n",
    "        # Callbacks support token-wise streaming\n",
    "        callbacks = [StreamingStdOutCallbackHandler()]\n",
    "        # Verbose is required to pass to the callback manager\n",
    "\n",
    "        self.llm = GPT4All(model=self.model_path, callbacks=callbacks, verbose=False,\n",
    "                           n_threads=n_threads, n_predict=max_tokens, repeat_penalty=repeat_penalty, \n",
    "                           n_batch=n_batch, top_k=top_k, temp=temp)\n",
    "\n",
    "    def build_vectordb(self, chunk_size, overlap):\n",
    "        \"\"\"\n",
    "        Builds a vector database from the dataset for retrieval purposes.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        chunk_size : int\n",
    "            The size of text chunks for vectorization.\n",
    "        overlap : int\n",
    "            The overlap size between chunks.\n",
    "        \"\"\"\n",
    "        loader = TextLoader(self.data_path)\n",
    "        # Text Splitter\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=overlap)\n",
    "        # Embed the document and store into chroma DB\n",
    "        self.index = VectorstoreIndexCreator(embedding= HuggingFaceEmbeddings(), text_splitter=text_splitter).from_loaders([loader])\n",
    "\n",
    "    def retrieval_mechanism(self, user_input, top_k=1, context_verbosity = False, rag_off= False):\n",
    "        \"\"\"\n",
    "        Retrieves relevant document snippets based on the user's query.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_input : str\n",
    "            The user's input or query.\n",
    "        top_k : int, optional\n",
    "            The number of top results to return, by default 1.\n",
    "        context_verbosity : bool, optional\n",
    "            If True, additional context information is printed, by default False.\n",
    "        rag_off : bool, optional\n",
    "            If True, disables the retrieval-augmented generation, by default False.\n",
    "        \"\"\"\n",
    "\n",
    "        self.user_input = user_input\n",
    "        self.context_verbosity = context_verbosity\n",
    "                \n",
    "        # perform a similarity search and retrieve the context from our documents\n",
    "        results = self.index.vectorstore.similarity_search(self.user_input, k=top_k)\n",
    "        # join all context information into one string \n",
    "        context = \"\\n\".join([document.page_content for document in results])\n",
    "        if self.context_verbosity:\n",
    "            print(f\"Retrieving information related to your question...\")\n",
    "            print(f\"Found this content which is most similar to your question: {context}\")\n",
    "\n",
    "        if rag_off:\n",
    "            template = \"\"\"Question: {question}\n",
    "            Answer: This is the response: \"\"\"\n",
    "            self.prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "        else:     \n",
    "            template = \"\"\" Don't just repeat the following context, use it in combination with your knowledge to improve your answer to the question:{context}\n",
    "\n",
    "            Question: {question}\n",
    "            \"\"\"\n",
    "            self.prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"]).partial(context=context)\n",
    "\n",
    "\n",
    "    def inference(self):\n",
    "        \"\"\"\n",
    "        Performs inference to generate a response based on the user's query.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The generated response.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.context_verbosity:\n",
    "            print(f\"Your Query: {self.prompt}\")\n",
    "            \n",
    "        llm_chain = LLMChain(prompt=self.prompt, llm=self.llm)\n",
    "        print(\"Processing the information with gpt4all...\\n\")\n",
    "        response = llm_chain.run(self.user_input)\n",
    "\n",
    "        return  response  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e592882e-3afb-4a83-a3e6-6f0f51176507",
   "metadata": {},
   "source": [
    "# Customizing Your RAG LLM Chatbot Experience\r\n",
    "\r\n",
    "Welcome to the RAG LLM Chatbot interface! This guide will help you understand how to use the various widgets to customize your chatbot experience.\r\n",
    "\r\n",
    "## Interface Elements\r\n",
    "\r\n",
    "### Model Selection\r\n",
    "- **Model Dropdown**: Choose the model for your chatbot.\r\n",
    "  - Available options include 'Falcon' and 'More Models Coming Soon!'.\r\n",
    "  - This dropdown allows you to select the underlying model your chatbot will use for generating responses.\r\n",
    "\r\n",
    "### Query Input\r\n",
    "- **Query Text Box**: Enter your query here.\r\n",
    "  - Type the question or statement you want the chatbot to respond to.\r\n",
    "\r\n",
    "### Response Customization\r\n",
    "- **Top K Slider**: Adjust the number of top results to consider for generating responses.\r\n",
    "  - Slide to increase or decrease the value. The range is from 1 to 4.\r\n",
    "  - This adjusts how many top possibilities the model considers when crafting a response.\r\n",
    "\r\n",
    "- **RAG OFF Checkbox**: Toggle whether to use Retrieval-Augmented Generation (RAG) or not.\r\n",
    "  - Check this box if you want to turn off RAG and use only the raw model and prompt for inference.\r\n",
    "\r\n",
    "### Data Processing Settings\r\n",
    "- **Chunk Size Input**: Set the size of text chunks for processing.\r\n",
    "  - Enter a value to determine how large each text chunk should be. The range is from 5 to 5000.\r\n",
    "  - This affects how the text data is segmented for analysis.\r\n",
    "\r\n",
    "- **Overlap Input**: Define the overlap size between chunks.\r\n",
    "  - Set a value for how much overlap there should be between text chunks. The range is from 0 to 1000.\r\n",
    "  - This can influence the continuity in data processing.\r\n",
    "\r\n",
    "### Dataset and Performance\r\n",
    "- **Dataset Dropdown**: Choose the dataset for training your model.\r\n",
    "  - Select from options like 'robot maintenance', 'basketball coach', 'physics professor', 'grocery cashier'.\r\n",
    "  - The dataset selected will be used to train and inform the chatbot's responses.\r\n",
    "\r\n",
    "- **Threads Slider**: Select the number of threads for parallel processing.\r\n",
    "  - Adjust the slider to set the number of threads. The range is from 2 to 200.\r\n",
    "  - This setting can affect the performance and speed of your model's processing.\r\n",
    "\r\n",
    "- **Max Tokens Input**: Specify the maximum number of tokens for model prediction.\r\n",
    "  - Enter a value to set the limit for the number of tokens. The range is from 5 to 500.\r\n",
    "  - This sets a cap on the length of the generated responses.\r\n",
    "\r\n",
    "### Submit Button\r\n",
    "- **Submit**: Click this button to process your input and generate a response.\r\n",
    "  - Once you have set all your parameters, click here to see the chatbot in action!\r\n",
    "\r\n",
    "## How to Use\r\n",
    "1. Select your desired model from the **Model Dropdown**.\r\n",
    "2. Type your query in the **Query Text Box**.\r\n",
    "3. Adjust the **Top K Slider** to set the number of top results for response generation.\r\n",
    "4. Check or uncheck the **RAG OFF Checkbox** based on your preference.\r\n",
    "5. Set the **Chunk Size** and **Overlap** for data processing.\r\n",
    "6. Choose a dataset from the **Dataset Dropdown**.\r\n",
    "7. Adjust the **Threads Slider** and **Max Tokens Input** for performance.\r\n",
    "8\n",
    "\n",
    "*The first execution will load the model into memory, adding additional overhead. The model will not be reloaded unless you change model specific initialization parameters like threads or max tokens.*\n",
    ". Click **Submit** to generate and display the response.\r\n",
    "\r\n",
    "Enjoy interacting with your custom RAG LLM Chatbot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40f6848b-5cef-4f45-a2c1-97db53e29ba3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc5db778c463445ba7021a283de6b25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(VBox(children=(Dropdown(description='Model:', options=('Falcon', 'More Models Coming Soon!'), v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9cbe2429bff455d9d388c819b657093",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "bot = RAGBot()\n",
    "\n",
    "# Initialize previous value variables\n",
    "previous_threads = None\n",
    "previous_max_tokens = None\n",
    "previous_top_k = None\n",
    "previous_dataset = None\n",
    "previous_chunk_size = None\n",
    "previous_overlap = None\n",
    "previous_temp = None\n",
    "\n",
    "# Create an output widget\n",
    "output = widgets.Output()\n",
    "\n",
    "def process_inputs(b):\n",
    "    \"\"\"\n",
    "    Process inputs from the interactive chat interface.\n",
    "\n",
    "    This function is triggered by a button click in the IPython widgets interface. It captures \n",
    "    user inputs from various widget elements, such as dropdowns, sliders, and text inputs. The function \n",
    "    handles model and dataset downloading, initiates model loading and vector database building, \n",
    "    performs the retrieval mechanism, and generates a response to the user's query. The response is \n",
    "    then displayed in a styled HTML format within the Jupyter Notebook.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    b : ipywidgets.widgets.widget_button.Button\n",
    "        The button widget that triggers this function. This parameter is required by the\n",
    "        widget framework but is not directly used in the function.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - This function is designed to be used as a callback for an IPython button widget.\n",
    "    - It utilizes global variables to access and update the widget elements and their values.\n",
    "    - The function updates global variables to keep track of previous parameter values for \n",
    "      efficient reloading of models and rebuilding of vector databases.\n",
    "    - Standard output and error output are captured and redirected to suppress unnecessary console logs,\n",
    "      while relevant output is displayed via the IPython display mechanism.\n",
    "    \"\"\"\n",
    "    global previous_threads, previous_max_tokens, previous_top_k, previous_dataset, previous_chunk_size, previous_overlap, previous_temp\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        # Suppress output\n",
    "        f = io.StringIO()\n",
    "        with contextlib.redirect_stdout(f), contextlib.redirect_stderr(f):\n",
    "\n",
    "            # Function to process inputs\n",
    "            # Gather values from the widgets\n",
    "            model = model_dropdown.value\n",
    "            query = query_text.value\n",
    "            top_k = top_k_slider.value\n",
    "            chunk_size = chunk_size_input.value\n",
    "            overlap = overlap_input.value\n",
    "            dataset = dataset_dropdown.value\n",
    "            threads = threads_slider.value\n",
    "            max_tokens = max_token_input.value\n",
    "            rag_off = rag_off_checkbox.value\n",
    "            temp = temp_slider.value\n",
    "            bot.get_model(model = model)\n",
    "            bot.download_dataset(dataset = dataset)\n",
    "            if threads != previous_threads or max_tokens != previous_max_tokens or top_k != previous_top_k or temp != previous_temp:\n",
    "                print(\"loading model due incorporate new parameters\")\n",
    "                bot.load_model(n_threads=threads, max_tokens=max_tokens, repeat_penalty=1.50, n_batch=threads, top_k=top_k, temp=temp)\n",
    "                # Update previous values\n",
    "                previous_threads = threads\n",
    "                previous_max_tokens = max_tokens\n",
    "                previous_top_k = top_k\n",
    "                previous_temp = temp\n",
    "            if dataset != previous_dataset or chunk_size != previous_chunk_size or overlap != previous_overlap:\n",
    "                print(\"rebuilding vector DB due to changing dataset, overlap, or chunk\")\n",
    "                bot.build_vectordb(chunk_size = chunk_size, overlap = overlap)\n",
    "                previous_dataset = dataset\n",
    "                previous_chunk_size = chunk_size\n",
    "                previous_overlap = overlap\n",
    "            bot.retrieval_mechanism(user_input = query, rag_off = rag_off)\n",
    "            response = bot.inference()\n",
    "    \n",
    "            styled_response = f\"\"\"\n",
    "            <div style=\"\n",
    "                background-color: lightblue;\n",
    "                border-radius: 15px;\n",
    "                padding: 10px;\n",
    "                font-family: Arial, sans-serif;\n",
    "                color: black;\n",
    "                max-width: 600px;\n",
    "                word-wrap: break-word;\n",
    "                margin: 10px;\n",
    "                font-size: 14px;\">\n",
    "                {response}\n",
    "            </div>\n",
    "            \"\"\"\n",
    "            display(HTML(styled_response))\n",
    "\n",
    "def create_chat_interface():\n",
    "    global model_dropdown, query_text, top_k_slider, rag_off_checkbox, chunk_size_input, overlap_input, dataset_dropdown, threads_slider, max_token_input, repeat_penalty_input, temp_slider\n",
    "    # Model selection dropdown\n",
    "    model_dropdown = widgets.Dropdown(\n",
    "        options=['Falcon', 'More Models Coming Soon!'],\n",
    "        description='Model:',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    # User query text input\n",
    "    query_layout = widgets.Layout(width='400px', height='400px')  # Adjust the width as needed\n",
    "    query_text = widgets.Text(\n",
    "        placeholder='Type your query here',\n",
    "        description='Query:',\n",
    "        disabled=False, \n",
    "        layout=query_layout\n",
    "    )\n",
    "\n",
    "    # Vector search top k slider\n",
    "    top_k_slider = widgets.IntSlider(\n",
    "        value=2,\n",
    "        min=1,\n",
    "        max=4,\n",
    "        step=1,\n",
    "        description='Top K:',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='d'\n",
    "    )\n",
    "\n",
    "    # Model Temperature slider\n",
    "    temp_slider = widgets.FloatSlider(\n",
    "    value=0.7,\n",
    "    min=0.1,\n",
    "    max=1.4,\n",
    "    step=0.1,\n",
    "    description='Temperature:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='.1f'\n",
    ")\n",
    "    \n",
    "    # RAG OFF TOGGLE\n",
    "    rag_off_checkbox = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='RAG OFF?',\n",
    "    disabled=False,\n",
    "    indent=False,  # Set to True if you want the checkbox to be indented\n",
    "    tooltip='Turns off RAG and Performs Inference with Raw Model and Prompt Only'\n",
    "    )\n",
    "\n",
    "    # Chunk size number input\n",
    "    chunk_size_input = widgets.BoundedIntText(\n",
    "        value=500,\n",
    "        min=5,\n",
    "        max=5000,\n",
    "        step=1,\n",
    "        description='Chunk Size:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    # Overlap number input\n",
    "    overlap_input = widgets.BoundedIntText(\n",
    "        value=50,\n",
    "        min=0,\n",
    "        max=1000,\n",
    "        step=1,\n",
    "        description='Overlap:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    # Dataset selection dropdown\n",
    "    dataset_dropdown = widgets.Dropdown(\n",
    "        options=['robot maintenance', 'basketball coach', 'physics professor', 'grocery cashier'],\n",
    "        description='Dataset:',\n",
    "        disabled=False,\n",
    "    )\n",
    "\n",
    "    # Number of threads slider\n",
    "    threads_slider = widgets.IntSlider(\n",
    "        value=64,\n",
    "        min=2,\n",
    "        max=200,\n",
    "        step=1,\n",
    "        description='Threads:',\n",
    "        disabled=False,\n",
    "        continuous_update=False,\n",
    "        orientation='horizontal',\n",
    "        readout=True,\n",
    "        readout_format='d'\n",
    "    )\n",
    "\n",
    "    # Max token number input\n",
    "    max_token_input = widgets.BoundedIntText(\n",
    "        value=50,\n",
    "        min=5,\n",
    "        max=500,\n",
    "        step=5,\n",
    "        description='Max Tokens:',\n",
    "        disabled=False\n",
    "    )\n",
    "\n",
    "    # Group the widgets except the query text into a VBox\n",
    "    left_column = widgets.VBox([model_dropdown, top_k_slider, temp_slider, rag_off_checkbox, chunk_size_input, \n",
    "                                overlap_input, dataset_dropdown, threads_slider, max_token_input])\n",
    "\n",
    "    # Submit button\n",
    "    submit_button = widgets.Button(description=\"Submit\")\n",
    "    submit_button.on_click(process_inputs)\n",
    "\n",
    "    right_column = widgets.VBox([query_text, submit_button])\n",
    "\n",
    "    # Use HBox to position the VBox and query text side by side\n",
    "    interface_layout = widgets.HBox([left_column, right_column])\n",
    "\n",
    "\n",
    "    # Display the layout\n",
    "    display(interface_layout, output)\n",
    "\n",
    "create_chat_interface()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ecd5c8-f328-46a4-95a4-46ebcd9d2190",
   "metadata": {},
   "source": [
    "## Disclaimer for Using Large Language Models\n",
    "\n",
    "Please be aware that while Large Language Models like Falcon are powerful tools for text generation, they may sometimes produce results that are unexpected, biased, or inconsistent with the given prompt. It's advisable to carefully review the generated text and consider the context and application in which you are using these models.\n",
    "\n",
    "Usage of these models must also adhere to the licensing agreements and be in accordance with ethical guidelines and best practices for AI. If you have any concerns or encounter issues with the models, please refer to the respective model cards and documentation provided in the links above.\n",
    "\n",
    "To the extent that any public or non-Intel datasets or models are referenced by or accessed using these materials those datasets or models are provided by the third party indicated as the content source. Intel does not create the content and does not warrant its accuracy or quality. By accessing the public content, or using materials trained on or with such content, you agree to the terms associated with that content and that your use complies with the applicable license.\n",
    "\n",
    " \n",
    "Intel expressly disclaims the accuracy, adequacy, or completeness of any such public content, and is not liable for any errors, omissions, or defects in the content, or for any reliance on the content. Intel is not liable for any liability or damages relating to your use of public content.\n",
    "\n",
    "Intel’s provision of these resources does not expand or otherwise alter Intel’s applicable published warranties or warranty disclaimers for Intel products or solutions, and no additional obligations, indemnifications, or liabilities arise from Intel providing such resources. Intel reserves the right, without notice, to make corrections, enhancements, improvements, and other changes to its materials."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
