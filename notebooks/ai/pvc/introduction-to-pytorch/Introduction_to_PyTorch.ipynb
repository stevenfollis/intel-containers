{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc43186-0e67-4774-9214-405bbe777881",
   "metadata": {},
   "source": [
    "## Getting Started with PyTorch on Intel GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5010643-e3f8-4a49-8e06-7d4ba8523ff2",
   "metadata": {},
   "source": [
    "### Introduction:\n",
    "\n",
    "Welcome to this Jupyter Notebook tutorial on getting started with PyTorch on Intel GPUs! In this notebook, we'll explore the exciting new features and capabilities of PyTorch, specifically the support for Intel GPUs (XPUs). We've already set up the environment and installed PyTorch for you, so you can dive right into learning and experimenting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9690574a-66e1-4853-a29c-66906975ea07",
   "metadata": {},
   "source": [
    "> **Note**: Ensure that you have selected `pytorch_2.5` as the Jupyter kernel before proceeding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72cad05-986e-4d74-97e8-207b70c6e3cc",
   "metadata": {},
   "source": [
    "Throughout this notebook, we'll cover the basics of tensor operations, demonstrate how to check the device being used, and walk through a few example workloads to showcase the power of PyTorch on Intel GPUs. Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87cfa1a-cc62-4b3d-aaa3-bd3ca50c0825",
   "metadata": {},
   "source": [
    "**Step 1**: Checking PyTorch Version and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a20d2082-6ab3-429a-8c99-e3bc0bc4f0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.5.0.dev20240911+xpu\n",
      "Using device: xpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"ZE_ENABLE_TRACING_LAYER\"] = \"1\"\n",
    "\n",
    "import torch\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "device = torch.device('xpu' if torch.xpu.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a391c774-b166-408c-9b76-ef5b51f47ccc",
   "metadata": {},
   "source": [
    "**Step 2**: Let's see a basic Tensor Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95db2f0f-5497-4eb7-a621-9c4048f7e548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: xpu\n",
      "Tensor: tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], device='xpu:0')\n",
      "Tensor device: xpu:0\n",
      "Matrix multiplication result shape: torch.Size([3, 5])\n"
     ]
    }
   ],
   "source": [
    "# device selection\n",
    "device = torch.device('xpu' if torch.xpu.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a tensor on the XPU device\n",
    "tensor = torch.ones(3, 4, device=device)\n",
    "print(f\"Tensor: {tensor}\")\n",
    "print(f\"Tensor device: {tensor.device}\")\n",
    "\n",
    "# Matrix multiplication\n",
    "mat1 = torch.randn(3, 4, device=device)\n",
    "mat2 = torch.randn(4, 5, device=device)\n",
    "result = torch.matmul(mat1, mat2)\n",
    "print(f\"Matrix multiplication result shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcff4336-346c-46ac-83e7-699affa2c7ec",
   "metadata": {},
   "source": [
    "**Step 3**: Example Workload - Image Classification with FP32 precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d025b90d-7845-4629-8828-61ba5f01c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: xpu\n",
      "Predicted class: 285\n",
      "Class label: Egyptian cat\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import requests\n",
    "from torchvision.models.resnet import ResNet18_Weights\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# device selection\n",
    "device = torch.device('xpu' if torch.xpu.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Get model\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "model = models.resnet18(weights=weights)\n",
    "model = model.to(device)\n",
    "imagenet_classes = weights.meta[\"categories\"]\n",
    "\n",
    "# Prepare the input image\n",
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Cat_November_2010-1a.jpg/1200px-Cat_November_2010-1a.jpg\"\n",
    "input_image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "input_tensor = preprocess(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "# infer\n",
    "model = model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(input_batch)\n",
    "\n",
    "_, predicted = torch.max(output, 1)\n",
    "class_index = predicted.item()\n",
    "class_label = imagenet_classes[class_index]\n",
    "\n",
    "print(f\"Predicted class: {class_index}\")\n",
    "print(f\"Class label: {class_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c051f2f4-abb5-496e-bdd4-a46e048559cf",
   "metadata": {},
   "source": [
    "We used an image of a cat, but if you are not getting the `Class label` as a cat it is just a reminder that even the best models can benefit from a little fine-tuning to help them stay on track. With a bit of training, we can help ResNet18 regain its cat-detecting superpowers and avoid any future blunders! ðŸ˜º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509ecba5-a819-4d51-9fb5-72be31081813",
   "metadata": {},
   "source": [
    "**Step 4**: Example Workload - Sentiment Analysis inference\n",
    "\n",
    "Let's see another example with LSTMs and inference after model compilation using `torch.compile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3263b8bf-f2d0-4b7c-bd20-5a07e9df99df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: xpu\n",
      "\n",
      "Model before compilation: \n",
      "SentimentModel(\n",
      "  (embedding): Embedding(10000, 100)\n",
      "  (lstm): LSTM(100, 512, batch_first=True)\n",
      "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "------------------------------------------------------------------------\n",
      "\n",
      "Model after compilation: \n",
      "OptimizedModule(\n",
      "  (_orig_mod): SentimentModel(\n",
      "    (embedding): Embedding(10000, 100)\n",
      "    (lstm): LSTM(100, 512, batch_first=True)\n",
      "    (fc): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "Sentiment score: 1\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# device selection\n",
    "device = torch.device('xpu' if torch.xpu.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define a simple sentiment analysis model (expected to have a trained model, for now we will use this model as an example)\n",
    "class SentimentModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(x)\n",
    "        out = self.fc(hidden.squeeze(0))\n",
    "        return out\n",
    "\n",
    "\n",
    "vocab_size = 10000\n",
    "embed_dim = 100\n",
    "hidden_dim = 512\n",
    "output_dim = 2\n",
    "\n",
    "model = SentimentModel(vocab_size, embed_dim, hidden_dim, output_dim)\n",
    "model = model.to(device)\n",
    "print(f\"\\nModel before compilation: \\n{model}\\n\")\n",
    "model = torch.compile(model)  # compile model\n",
    "print(\"-\"*72)\n",
    "print(f\"\\nModel after compilation: \\n{model}\")\n",
    "\n",
    "input_text = torch.randint(0, vocab_size, (1, 20), device=device)\n",
    "output = model(input_text)\n",
    "sentiment = torch.argmax(output, dim=1)\n",
    "print(f\"Sentiment score: {sentiment.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368eda97-4cdf-4a44-a587-40306f42e508",
   "metadata": {},
   "source": [
    "**Step 5**: Transfer Learning - Vision (ResNet18) using Auto Mixed Precision (we can use torch.float16 or torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef75f4f8-1eef-4ecc-ba68-80e295aab3b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: xpu\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Initiating training: Epoch 0\n",
      "Iteration [10/391], Loss: 1.2851\n",
      "Iteration [20/391], Loss: 0.8821\n",
      "Iteration [30/391], Loss: 0.6356\n",
      "Iteration [40/391], Loss: 0.6047\n",
      "Iteration [50/391], Loss: 0.5314\n",
      "Iteration [60/391], Loss: 0.4241\n",
      "Iteration [70/391], Loss: 0.4907\n",
      "Iteration [80/391], Loss: 0.3260\n",
      "Iteration [90/391], Loss: 0.2822\n",
      "Iteration [100/391], Loss: 0.2285\n",
      "Iteration [110/391], Loss: 0.3030\n",
      "Iteration [120/391], Loss: 0.3085\n",
      "Iteration [130/391], Loss: 0.2870\n",
      "Iteration [140/391], Loss: 0.3984\n",
      "Iteration [150/391], Loss: 0.2316\n",
      "Iteration [160/391], Loss: 0.2748\n",
      "Iteration [170/391], Loss: 0.2755\n",
      "Iteration [180/391], Loss: 0.2155\n",
      "Iteration [190/391], Loss: 0.3599\n",
      "Iteration [200/391], Loss: 0.1832\n",
      "Iteration [210/391], Loss: 0.2960\n",
      "Iteration [220/391], Loss: 0.1651\n",
      "Iteration [230/391], Loss: 0.2840\n",
      "Iteration [240/391], Loss: 0.1997\n",
      "Iteration [250/391], Loss: 0.1497\n",
      "Iteration [260/391], Loss: 0.2326\n",
      "Iteration [270/391], Loss: 0.2497\n",
      "Iteration [280/391], Loss: 0.1819\n",
      "Iteration [290/391], Loss: 0.2967\n",
      "Iteration [300/391], Loss: 0.2607\n",
      "Iteration [310/391], Loss: 0.1341\n",
      "Iteration [320/391], Loss: 0.1477\n",
      "Iteration [330/391], Loss: 0.1338\n",
      "Iteration [340/391], Loss: 0.2185\n",
      "Iteration [350/391], Loss: 0.3033\n",
      "Iteration [360/391], Loss: 0.2007\n",
      "Iteration [370/391], Loss: 0.2055\n",
      "Iteration [380/391], Loss: 0.1841\n",
      "Iteration [390/391], Loss: 0.2812\n",
      "Epoch 1 completed, Epoch Loss: 0.3422\n",
      "Accuracy on test images: 93.66%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from PIL import Image\n",
    "import requests\n",
    "import tqdm\n",
    "\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "# device selection\n",
    "device = 'xpu' if torch.xpu.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Use CIFAR10 dataset \n",
    "train_dataset = datasets.CIFAR10(root='~/data', train=True, download=True, transform=transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "]))\n",
    "test_dataset = datasets.CIFAR10(root='~/data', train=False, download=True, transform=transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "train_len = len(train_loader)\n",
    "\n",
    "# Load the pre-trained ResNet18 model and move it to an `xpu` device.\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "imagenet_classes = weights.meta[\"categories\"]\n",
    "model = models.resnet18(weights=weights)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 10) \n",
    "\n",
    "# optimizer and loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs = 1\n",
    "\n",
    "# set model to train and move model and criterion to `device`\n",
    "model = model.train()\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "# Training\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    print(f\"Initiating training: Epoch {epoch}\")\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        with torch.autocast(device_type=device, dtype=torch.bfloat16, enabled=True):  # using torch.bfloat16\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if (i + 1) % 10 == 0:\n",
    "            iteration_loss = loss.item()\n",
    "            print(f\"Iteration [{i+1}/{train_len}], Loss: {iteration_loss:.4f}\")\n",
    "    epoch_loss = running_loss / (i + 1)\n",
    "    print(f\"Epoch {epoch + 1} completed, Epoch Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "model = model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"Accuracy on test images: {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d669719-e885-4762-9655-7f3720752797",
   "metadata": {},
   "source": [
    "Congratulations! You've now explored PyTorch with Intel GPU support using this Jupyter Notebook. We covered the basics of tensor operations, checked the device being used, and walked through a couple of example workloads.\n",
    "\n",
    "Feel free to experiment further, modify the code snippets, and explore more advanced topics.\n",
    "\n",
    "For more information and examples on Pytorch on Intel GPUs, please refer to this [link](https://pytorch.org/docs/main/notes/get_start_xpu.html).\n",
    "\n",
    "Happy learning and coding with PyTorch on Intel GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46895b7-3586-4ca0-93ff-c5f8cf5238a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.5",
   "language": "python",
   "name": "pytorch-2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
